{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Yk2djzoPdkchhdf_GuLAbaFDHUA0Y14X",
      "authorship_tag": "ABX9TyODv325kz88cgVjvzBCpI2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachindamle/Resources/blob/main/LangChainTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install\n",
        "!pip install -qU openai\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -qU langchain\n",
        "!pip install -qU huggingface_hub\n",
        "!pip install -qU langchain-community\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU chromadb\n",
        "!pip install -qU tiktoken"
      ],
      "metadata": {
        "id": "BHyDeMz4c8qP"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')"
      ],
      "metadata": {
        "id": "9Zhbs9zAc0zS"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "sC5XQHVqgkpZ"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxamWT6EeAeQ",
        "outputId": "4720f92b-1eb6-4bfb-d4d6-1638e5304815"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_pdf = !ls /content/drive/MyDrive/Learnings/ML4T\\ Notes.pdf\n",
        "path_to_pdf=path_to_pdf[0]\n",
        "print(path_to_pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcKTKktre7Or",
        "outputId": "ee3cb482-213d-48f4-c48f-8abf46a9a7a1"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/Learnings/ML4T Notes.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "3lR18oy8jiNu"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Learnings/ML4T Notes.pdf\")\n",
        "\n",
        "#Load the document by calling loader.load()\n",
        "pages = loader.load()\n",
        "\n",
        "print(len(pages))\n",
        "#print(pages[0].page_content[0:500])\n",
        "\n",
        "#print(pages[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0uTD6glgUXu",
        "outputId": "f9194830-dcc3-433b-9138-fca454e8a5d2"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "splits = rsplitter.split_documents(pages)\n",
        "print(len(splits))\n",
        "print(len(pages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPa3R1ASgdA5",
        "outputId": "47701fa4-402d-4319-b343-56673163ddc4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "272\n",
            "148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "vH011h5WlssN"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector store\n",
        "persist_directory = 'docs/chroma/'\n",
        "vectorDB = Chroma.from_documents(documents=splits,\n",
        "                                     embedding=embeddings,\n",
        "                                     persist_directory=persist_directory)\n",
        "print(vectorDB._collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3R58MgMmlSn",
        "outputId": "30987e79-dc49-4298-ccff-c12ef14aff32"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(model_name=model_name, temperature=0)"
      ],
      "metadata": {
        "id": "_e4Gl_pNtfe3"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "#from getpass import getpass\n",
        "\n",
        "#api_key = getpass()\n",
        "#llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=api_key)"
      ],
      "metadata": {
        "id": "aaYWacVB31Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "#HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n"
      ],
      "metadata": {
        "id": "6ecph0cA7_Cz"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
        "qaChain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=vectorDB.as_retriever(),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "    )"
      ],
      "metadata": {
        "id": "Oj2AKDR4vVz5"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AskMe(question):\n",
        "  result = qaChain({\"query\": question})\n",
        "  return result[\"result\"]"
      ],
      "metadata": {
        "id": "G2x-nGeIxNRr"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass question to the qa_chain\n",
        "question = \"What are major topics for this class?\"\n",
        "print(AskMe(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcuWRHxvzQr",
        "outputId": "54f616dc-db6d-49a6-8786-b8caa1ad558d"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Manipulating Financial Data in Python  Read historical financial data into python and manipulate it using powerful statistical algorithms 2. Computational Investing  Algorithms, methods and models used by hedge funds and investment banks to manipulate and work with financial data 3. Learning Algorithms for Trading  We pull everything together:  Take what we learned in the first two mini courses \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass question to the qa_chain\n",
        "question = \"Summarize the document in 5000 words\"\n",
        "print(AskMe(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHhbBvQ_wz1S",
        "outputId": "ce5e2510-8d72-444c-8947-69e8b21c45e9"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass question to the qa_chain\n",
        "question = \"How machine learning can help trading?\"\n",
        "print(AskMe(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSYF69U3x5AQ",
        "outputId": "ca924daf-4ad7-498f-bf90-e7ff65a63cdc"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning can help trading by creating a model that can be used to predict future prices for stocks or other assets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass question to the qa_chain\n",
        "question = \"Who is Sachin Tendulkar?\"\n",
        "print(AskMe(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaLS-vzEyCTm",
        "outputId": "37b1a481-ca55-4423-a04b-4f5ed3dfe018"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sachin Tendulkar is a cricketer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass question to the qa_chain\n",
        "question = \"What is the document about?\"\n",
        "print(AskMe(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJP7DJ3vAris",
        "outputId": "c413e013-adfe-432d-95a0-75f6fe3d8688"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document is about options.\n"
          ]
        }
      ]
    }
  ]
}